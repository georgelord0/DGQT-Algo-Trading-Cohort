{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Calculating Risk (Variance/Standard Deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, **variance** is a measurement of how far each number in a data set is from the mean (average), and thus from every other number in the set. This can be thought of how much we can expect values in the data set to differ from eachother. \n",
    "\n",
    "**Standard Deviation** is the square root of the variance, meaning it is expressed in the same units as the original data (variance is expressed in squared units).\n",
    "\n",
    "In quantitative finance, **volatility** is used as a statistical measure of risk in an asset's returns. It is often measured from either the standard deviation or variance in returns.\n",
    "\n",
    "In the context of quantitative finance, the **volatility of rrturns** is used as a measure of **risk**. With higher volatility, the returns of an asset at different times vary more, and vice-versa. As such, higher volatility is associated with a greater possibility for larger gains or losses, and vice versa. Thus, high volatility in a stock's returns is associated with higher risk along with higher return.\n",
    "\n",
    "Let's calculate the volatility of a stock (using standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker = \"MSFT\"  # Microsoft\n",
    "data = yf.download(stock_ticker, start=\"2024-01-01\", end=\"2025-01-03\")  # Get last year's daily price data for MSFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to know the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Pct Return\"] = data['Close'].pct_change()  # Calculate daily percent return\n",
    "data.dropna(inplace=True)  # Drop the first row since it will have a NaN value\n",
    "data[\"Pct Return\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the standard deviation of these returns to get the *daily* volatility over the given timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_vol = data[\"Pct Return\"].std()  # Calculate daily volatility\n",
    "print(daily_vol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, let's get the average daily return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean_returns = data[\"Pct Return\"].mean()  # Calculate daily mean returns\n",
    "print(daily_mean_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Annualizing Returns and Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our returns and volatility are measured on a daily time frame. Let's get the returns and volatility **annualized** over the year.\n",
    "\n",
    "Since there are 252 *trading days* in a year (typically, RIP Jimmy Carter), we need to multiply the average daily return by 252. However, since standard deviation is the *square root* of variance, we need to multiply the variance by the *square root of 252*.\n",
    "\n",
    "If we had monthly returns and volatility, we would similarly multiply them by 12 and root(12), since there are 12 months of trading in a year, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Close\"].shape)  # Confirming there are 252 trading days in our timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns_annualized = daily_mean_returns * 252  # Annualize mean returns\n",
    "volatility_annualized = daily_vol * np.sqrt(252)  # Annualize volatility\n",
    "print(mean_returns_annualized, volatility_annualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculating Risk Free Rate, Excess Returns, and Sharpe Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use U.S. T-bills (13 week) as the risk free asset. **^IRX** tracks the returns of these, so we can get that data to calculate our risk free rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbill_data = yf.download(\"^IRX\", start=\"2024-01-01\", end=\"2025-01-03\")  # Get last year's T-bill data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: These \"Close\" values are the **ANNUALIZED YIELDS** of the 13-week T-bill, which means we can use these numbers directly as the percentage return an investor would recieve by buying them on that date and holding until maturity (repeated) for a year. \n",
    "\n",
    "Let's get the yield for the start of 2024. (We are simplifying a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = float(tbill_data['Close'].iloc[0]) / 100 # Get the risk-free rate from the first row, scaled\n",
    "risk_free_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate MSFT's excess annualized returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_annualized_return = mean_returns_annualized - risk_free_rate  # Calculate excess returns\n",
    "print(excess_annualized_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now calculate the sharpe ratio, which is the annualized excess returns divided by the annualized volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe = excess_annualized_return / volatility_annualized  # Calculate Sharpe ratio\n",
    "sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculating Other Performance Metrict (MDD, Sortino, and VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Max Drawdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the **drawdowns** (movements from a peak to a through) of MSFT during the year on a rolling basis. To do this, we first need to calculate the rolling cummulative returns (returns from start to a given date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns = (1 + data[\"Pct Return\"]).cumprod()  # Calculate cumulative returns\n",
    "cum_max = cum_returns.cummax()  # Calculate running maximum returns for calculating drawdowns\n",
    "drawdown = (cum_returns - cum_max) / cum_max  # Calculate drawdowns on a rolling basis\n",
    "\n",
    "drawdown.plot()  # Plot drawdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the drawdowns at different points in the year. The **Max Drawdown** in our yearly period is simply the worst of these. This is the worst-case possible timing of investing (long) in MSFT. Visually you can see this would be buying around July '24 and selling August '24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_drawdown = drawdown.min()  # Calculate maximum drawdown\n",
    "print(max_drawdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Sortino Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sortino Ratio** is a lot like the sharpe ratio, but only considering standard deviation of *negative* asset returns (since big sharpe jumps upwards in returns are generally better welcomed :)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_deviation = data[\"Pct Return\"][data[\"Pct Return\"] < 0].std()  # Calculate downside deviation\n",
    "annualized_downside_deviation = downside_deviation * np.sqrt(252)  # Annualize downside deviation\n",
    "sortino = excess_annualized_return / annualized_downside_deviation  # Calculate Sortino ratio\n",
    "print(sortino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate Value at Risk, we simply take the 5th quantile of our historic returns. This gives us the \"minimum worst 5% of daily returns\" we can assume in the future, *assuming that historic returns predict future returns and returns are normally distributed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = data[\"Pct Return\"].quantile(0.05) # Calculate VaR\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we can expect to lose at least 2.1% on the worst 5% of days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: Cummulative Value at Risk (CVaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cummulative Value at Risk is simply the mean of the fifth quantile of historic returns. This is more like the expected value of our returns given that the returns are within the 5% worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar = data[\"Pct Return\"][data[\"Pct Return\"] <= var].mean()  # Calculate CVaR\n",
    "print(cvar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
